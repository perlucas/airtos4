{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ec02f4-b8ec-446a-971c-a1d0bf0997f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 23.3 from /opt/conda/lib/python3.11/site-packages/pip (python 3.11)\n",
      "Collecting setuptools<70\n",
      "  Obtaining dependency information for setuptools<70 from https://files.pythonhosted.org/packages/f7/29/13965af254e3373bceae8fb9a0e6ea0d0e571171b80d6646932131d6439b/setuptools-69.5.1-py3-none-any.whl.metadata\n",
      "  Downloading setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Downloading setuptools-69.5.1-py3-none-any.whl (894 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.6/894.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 74.1.2\n",
      "    Uninstalling setuptools-74.1.2:\n",
      "      Removing file or directory /opt/conda/lib/python3.11/site-packages/_distutils_hack/\n",
      "      Removing file or directory /opt/conda/lib/python3.11/site-packages/distutils-precedence.pth\n",
      "      Removing file or directory /opt/conda/lib/python3.11/site-packages/pkg_resources/\n",
      "      Removing file or directory /opt/conda/lib/python3.11/site-packages/setuptools-74.1.2.dist-info/\n",
      "      Removing file or directory /opt/conda/lib/python3.11/site-packages/setuptools/\n",
      "      Successfully uninstalled setuptools-74.1.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-cpu-aws 2.14.0 requires keras<2.15,>=2.14.0, but you have keras 3.5.0 which is incompatible.\n",
      "tensorflow-cpu-aws 2.14.0 requires ml-dtypes==0.2.0, but you have ml-dtypes 0.4.1 which is incompatible.\n",
      "tensorflow-cpu-aws 2.14.0 requires tensorboard<2.15,>=2.14, but you have tensorboard 2.17.1 which is incompatible.\n",
      "tensorflow-cpu-aws 2.14.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.16.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed setuptools-69.5.1\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.11/site-packages (3.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.11/site-packages (from h5py) (1.26.4)\n",
      "Collecting tf-agents\n",
      "  Using cached tf_agents-0.19.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tf-keras\n",
      "  Using cached tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow>=2.15\n",
      "  Using cached tensorflow-2.17.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (4.1 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.1.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (62 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (19 kB)\n",
      "Collecting pandas-ta\n",
      "  Using cached pandas_ta-0.3.14b0-py3-none-any.whl\n",
      "Collecting absl-py>=0.6.1 (from tf-agents)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting cloudpickle>=1.3 (from tf-agents)\n",
      "  Using cached cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gin-config>=0.4.0 (from tf-agents)\n",
      "  Using cached gin_config-0.5.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gym<=0.23.0,>=0.17.0 (from tf-agents)\n",
      "  Using cached gym-0.23.0-py3-none-any.whl\n",
      "Collecting pillow (from tf-agents)\n",
      "  Using cached pillow-10.4.0-cp311-cp311-manylinux_2_28_aarch64.whl.metadata (9.2 kB)\n",
      "Collecting six>=1.10.0 (from tf-agents)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting protobuf>=3.11.3 (from tf-agents)\n",
      "  Using cached protobuf-5.28.1-cp38-abi3-manylinux2014_aarch64.whl.metadata (592 bytes)\n",
      "Collecting wrapt>=1.11.1 (from tf-agents)\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions==4.5.0 (from tf-agents)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting pygame==2.1.3 (from tf-agents)\n",
      "  Using cached pygame-2.1.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (9.3 kB)\n",
      "Collecting tensorflow-probability~=0.23.0 (from tf-agents)\n",
      "  Using cached tensorflow_probability-0.23.0-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow>=2.15)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow>=2.15)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow>=2.15)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow>=2.15)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow>=2.15)\n",
      "  Using cached h5py-3.11.0-cp311-cp311-linux_aarch64.whl\n",
      "Collecting libclang>=13.0.0 (from tensorflow>=2.15)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2014_aarch64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow>=2.15)\n",
      "  Using cached ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow>=2.15)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting packaging (from tensorflow>=2.15)\n",
      "  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting protobuf>=3.11.3 (from tf-agents)\n",
      "  Using cached protobuf-4.25.4-cp37-abi3-manylinux2014_aarch64.whl.metadata (541 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow>=2.15)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting setuptools (from tensorflow>=2.15)\n",
      "  Using cached setuptools-74.1.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow>=2.15)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow>=2.15)\n",
      "  Using cached grpcio-1.66.1-cp311-cp311-manylinux_2_17_aarch64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow>=2.15)\n",
      "  Using cached tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.2.0 (from tensorflow>=2.15)\n",
      "  Using cached keras-3.5.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow>=2.15)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (14 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (62 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow>=2.15)\n",
      "  Using cached wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting gym-notices>=0.0.4 (from gym<=0.23.0,>=0.17.0->tf-agents)\n",
      "  Using cached gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting rich (from keras>=3.2.0->tensorflow>=2.15)\n",
      "  Using cached rich-13.8.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.2.0->tensorflow>=2.15)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.2.0->tensorflow>=2.15)\n",
      "  Using cached optree-0.12.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (47 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow>=2.15)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow>=2.15)\n",
      "  Using cached idna-3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow>=2.15)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow>=2.15)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.18,>=2.17->tensorflow>=2.15)\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.18,>=2.17->tensorflow>=2.15)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.18,>=2.17->tensorflow>=2.15)\n",
      "  Using cached werkzeug-3.0.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting decorator (from tensorflow-probability~=0.23.0->tf-agents)\n",
      "  Using cached decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting dm-tree (from tensorflow-probability~=0.23.0->tf-agents)\n",
      "  Using cached dm_tree-0.1.8-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (1.9 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow>=2.15)\n",
      "  Using cached MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (3.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.2.0->tensorflow>=2.15)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich->keras>=3.2.0->tensorflow>=2.15)\n",
      "  Using cached pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow>=2.15)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tf_agents-0.19.0-py3-none-any.whl (1.4 MB)\n",
      "Using cached pygame-2.1.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (13.3 MB)\n",
      "Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Using cached tf_keras-2.17.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached tensorflow-2.17.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (223.9 MB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (14.2 MB)\n",
      "Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (15.6 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.66.1-cp311-cp311-manylinux_2_17_aarch64.whl (5.5 MB)\n",
      "Using cached keras-3.5.0-py3-none-any.whl (1.1 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2014_aarch64.whl (23.8 MB)\n",
      "Using cached ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (2.2 MB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached protobuf-4.25.4-cp37-abi3-manylinux2014_aarch64.whl (293 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
      "Using cached setuptools-74.1.2-py3-none-any.whl (1.3 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (4.8 MB)\n",
      "Using cached tensorflow_probability-0.23.0-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached wrapt-1.16.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (80 kB)\n",
      "Using cached packaging-24.1-py3-none-any.whl (53 kB)\n",
      "Using cached pillow-10.4.0-cp311-cp311-manylinux_2_28_aarch64.whl (4.4 MB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (136 kB)\n",
      "Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Using cached idna-3.9-py3-none-any.whl (71 kB)\n",
      "Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Using cached werkzeug-3.0.4-py3-none-any.whl (227 kB)\n",
      "Using cached wheel-0.44.0-py3-none-any.whl (67 kB)\n",
      "Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Using cached dm_tree-0.1.8-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (146 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.12.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (319 kB)\n",
      "Using cached rich-13.8.1-py3-none-any.whl (241 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (29 kB)\n",
      "Using cached pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, namex, libclang, gym-notices, gin-config, flatbuffers, dm-tree, wrapt, wheel, urllib3, tzdata, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, six, setuptools, pygments, pygame, protobuf, pillow, packaging, numpy, mdurl, MarkupSafe, markdown, idna, grpcio, gast, decorator, cloudpickle, charset-normalizer, certifi, absl-py, werkzeug, tensorflow-probability, requests, python-dateutil, optree, opt-einsum, ml-dtypes, markdown-it-py, h5py, gym, google-pasta, astunparse, tf-agents, tensorboard, rich, pandas, pandas-ta, keras, tensorflow, tf-keras\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2024.2\n",
      "    Uninstalling pytz-2024.2:\n",
      "      Successfully uninstalled pytz-2024.2\n",
      "  Attempting uninstall: namex\n",
      "    Found existing installation: namex 0.0.8\n",
      "    Uninstalling namex-0.0.8:\n",
      "      Successfully uninstalled namex-0.0.8\n",
      "  Attempting uninstall: libclang\n",
      "    Found existing installation: libclang 18.1.1\n",
      "    Uninstalling libclang-18.1.1:\n",
      "      Successfully uninstalled libclang-18.1.1\n",
      "  Attempting uninstall: gym-notices\n",
      "    Found existing installation: gym-notices 0.0.8\n",
      "    Uninstalling gym-notices-0.0.8:\n",
      "      Successfully uninstalled gym-notices-0.0.8\n",
      "  Attempting uninstall: gin-config\n",
      "    Found existing installation: gin-config 0.5.0\n",
      "    Uninstalling gin-config-0.5.0:\n",
      "      Successfully uninstalled gin-config-0.5.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 24.3.25\n",
      "    Uninstalling flatbuffers-24.3.25:\n",
      "      Successfully uninstalled flatbuffers-24.3.25\n",
      "  Attempting uninstall: dm-tree\n",
      "    Found existing installation: dm-tree 0.1.8\n",
      "    Uninstalling dm-tree-0.1.8:\n",
      "      Successfully uninstalled dm-tree-0.1.8\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.16.0\n",
      "    Uninstalling wrapt-1.16.0:\n",
      "      Successfully uninstalled wrapt-1.16.0\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.44.0\n",
      "    Uninstalling wheel-0.44.0:\n",
      "      Successfully uninstalled wheel-0.44.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2024.1\n",
      "    Uninstalling tzdata-2024.1:\n",
      "      Successfully uninstalled tzdata-2024.1\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: termcolor\n",
      "    Found existing installation: termcolor 2.4.0\n",
      "    Uninstalling termcolor-2.4.0:\n",
      "      Successfully uninstalled termcolor-2.4.0\n",
      "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
      "    Found existing installation: tensorflow-io-gcs-filesystem 0.37.1\n",
      "    Uninstalling tensorflow-io-gcs-filesystem-0.37.1:\n",
      "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.37.1\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.2\n",
      "    Uninstalling tensorboard-data-server-0.7.2:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 69.5.1\n",
      "    Uninstalling setuptools-69.5.1:\n",
      "      Successfully uninstalled setuptools-69.5.1\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.18.0\n",
      "    Uninstalling Pygments-2.18.0:\n",
      "      Successfully uninstalled Pygments-2.18.0\n",
      "  Attempting uninstall: pygame\n",
      "    Found existing installation: pygame 2.1.3\n",
      "    Uninstalling pygame-2.1.3:\n",
      "      Successfully uninstalled pygame-2.1.3\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.4\n",
      "    Uninstalling protobuf-4.25.4:\n",
      "      Successfully uninstalled protobuf-4.25.4\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 10.4.0\n",
      "    Uninstalling pillow-10.4.0:\n",
      "      Successfully uninstalled pillow-10.4.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: mdurl\n",
      "    Found existing installation: mdurl 0.1.2\n",
      "    Uninstalling mdurl-0.1.2:\n",
      "      Successfully uninstalled mdurl-0.1.2\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.5\n",
      "    Uninstalling MarkupSafe-2.1.5:\n",
      "      Successfully uninstalled MarkupSafe-2.1.5\n",
      "  Attempting uninstall: markdown\n",
      "    Found existing installation: Markdown 3.7\n",
      "    Uninstalling Markdown-3.7:\n",
      "      Successfully uninstalled Markdown-3.7\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.9\n",
      "    Uninstalling idna-3.9:\n",
      "      Successfully uninstalled idna-3.9\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.66.1\n",
      "    Uninstalling grpcio-1.66.1:\n",
      "      Successfully uninstalled grpcio-1.66.1\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.6.0\n",
      "    Uninstalling gast-0.6.0:\n",
      "      Successfully uninstalled gast-0.6.0\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 3.0.0\n",
      "    Uninstalling cloudpickle-3.0.0:\n",
      "      Successfully uninstalled cloudpickle-3.0.0\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.8.30\n",
      "    Uninstalling certifi-2024.8.30:\n",
      "      Successfully uninstalled certifi-2024.8.30\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 2.1.0\n",
      "    Uninstalling absl-py-2.1.0:\n",
      "      Successfully uninstalled absl-py-2.1.0\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 3.0.4\n",
      "    Uninstalling Werkzeug-3.0.4:\n",
      "      Successfully uninstalled Werkzeug-3.0.4\n",
      "  Attempting uninstall: tensorflow-probability\n",
      "    Found existing installation: tensorflow-probability 0.23.0\n",
      "    Uninstalling tensorflow-probability-0.23.0:\n",
      "      Successfully uninstalled tensorflow-probability-0.23.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: optree\n",
      "    Found existing installation: optree 0.12.1\n",
      "    Uninstalling optree-0.12.1:\n",
      "      Successfully uninstalled optree-0.12.1\n",
      "  Attempting uninstall: opt-einsum\n",
      "    Found existing installation: opt-einsum 3.3.0\n",
      "    Uninstalling opt-einsum-3.3.0:\n",
      "      Successfully uninstalled opt-einsum-3.3.0\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.4.1\n",
      "    Uninstalling ml-dtypes-0.4.1:\n",
      "      Successfully uninstalled ml-dtypes-0.4.1\n",
      "  Attempting uninstall: markdown-it-py\n",
      "    Found existing installation: markdown-it-py 3.0.0\n",
      "    Uninstalling markdown-it-py-3.0.0:\n",
      "      Successfully uninstalled markdown-it-py-3.0.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.11.0\n",
      "    Uninstalling h5py-3.11.0:\n",
      "      Successfully uninstalled h5py-3.11.0\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.23.0\n",
      "    Uninstalling gym-0.23.0:\n",
      "      Successfully uninstalled gym-0.23.0\n",
      "  Attempting uninstall: google-pasta\n",
      "    Found existing installation: google-pasta 0.2.0\n",
      "    Uninstalling google-pasta-0.2.0:\n",
      "      Successfully uninstalled google-pasta-0.2.0\n",
      "  Attempting uninstall: astunparse\n",
      "    Found existing installation: astunparse 1.6.3\n",
      "    Uninstalling astunparse-1.6.3:\n",
      "      Successfully uninstalled astunparse-1.6.3\n",
      "  Attempting uninstall: tf-agents\n",
      "    Found existing installation: tf-agents 0.19.0\n",
      "    Uninstalling tf-agents-0.19.0:\n",
      "      Successfully uninstalled tf-agents-0.19.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.17.1\n",
      "    Uninstalling tensorboard-2.17.1:\n",
      "      Successfully uninstalled tensorboard-2.17.1\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 13.8.1\n",
      "    Uninstalling rich-13.8.1:\n",
      "      Successfully uninstalled rich-13.8.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "  Attempting uninstall: pandas-ta\n",
      "    Found existing installation: pandas-ta 0.3.14b0\n",
      "    Uninstalling pandas-ta-0.3.14b0:\n",
      "      Successfully uninstalled pandas-ta-0.3.14b0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.5.0\n",
      "    Uninstalling keras-3.5.0:\n",
      "      Successfully uninstalled keras-3.5.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.17.0\n",
      "    Uninstalling tensorflow-2.17.0:\n",
      "      Successfully uninstalled tensorflow-2.17.0\n",
      "  Attempting uninstall: tf-keras\n",
      "    Found existing installation: tf_keras 2.17.0\n",
      "    Uninstalling tf_keras-2.17.0:\n",
      "      Successfully uninstalled tf_keras-2.17.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-cpu-aws 2.14.0 requires keras<2.15,>=2.14.0, but you have keras 3.5.0 which is incompatible.\n",
      "tensorflow-cpu-aws 2.14.0 requires ml-dtypes==0.2.0, but you have ml-dtypes 0.4.1 which is incompatible.\n",
      "tensorflow-cpu-aws 2.14.0 requires tensorboard<2.15,>=2.14, but you have tensorboard 2.17.1 which is incompatible.\n",
      "tensorflow-cpu-aws 2.14.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.16.0 which is incompatible.\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 absl-py-2.1.0 astunparse-1.6.3 certifi-2024.8.30 charset-normalizer-3.3.2 cloudpickle-3.0.0 decorator-5.1.1 dm-tree-0.1.8 flatbuffers-24.3.25 gast-0.6.0 gin-config-0.5.0 google-pasta-0.2.0 grpcio-1.66.1 gym-0.23.0 gym-notices-0.0.8 h5py-3.11.0 idna-3.9 keras-3.5.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-1.26.4 opt-einsum-3.3.0 optree-0.12.1 packaging-24.1 pandas-2.2.2 pandas-ta-0.3.14b0 pillow-10.4.0 protobuf-4.25.4 pygame-2.1.3 pygments-2.18.0 python-dateutil-2.9.0.post0 pytz-2024.2 requests-2.32.3 rich-13.8.1 setuptools-74.1.2 six-1.16.0 tensorboard-2.17.1 tensorboard-data-server-0.7.2 tensorflow-2.17.0 tensorflow-io-gcs-filesystem-0.37.1 tensorflow-probability-0.23.0 termcolor-2.4.0 tf-agents-0.19.0 tf-keras-2.17.0 typing-extensions-4.5.0 tzdata-2024.1 urllib3-2.2.3 werkzeug-3.0.4 wheel-0.44.0 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "#!sudo apt-get update\n",
    "#!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
    "!pip install --force-reinstall -v \"setuptools<70\"\n",
    "!HDF5_DIR=/usr/local pip install --no-binary=h5py h5py\n",
    "!HDF5_DIR=/usr/local pip install tf-agents tf-keras \"tensorflow>=2.15\" numpy pandas pandas-ta --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57e299b2-1de0-4fdb-9fd7-47092b7e4a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Actor Network\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, num_action_types, num_shares_options):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = layers.Dense(128, activation='relu')\n",
    "        self.fc2 = layers.Dense(128, activation='relu')\n",
    "        # Two output layers, one for action type, one for number of shares\n",
    "        self.action_type_logits = layers.Dense(num_action_types)\n",
    "        self.num_shares_logits = layers.Dense(num_shares_options)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        # Output logits for both action type and number of shares\n",
    "        action_type_logits = self.action_type_logits(x)\n",
    "        num_shares_logits = self.num_shares_logits(x)\n",
    "        return action_type_logits, num_shares_logits\n",
    "\n",
    "\n",
    "# Critic Network\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = layers.Dense(128, activation='relu')\n",
    "        self.fc2 = layers.Dense(128, activation='relu')\n",
    "        self.value = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        return self.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da624ae-2494-405b-85b0-06fada1750e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3CAgent:\n",
    "    def __init__(self, action_spec, observation_spec, lr=1e-4):\n",
    "        self.actor = Actor()\n",
    "        self.critic = Critic()\n",
    "\n",
    "        # Optimizers for both actor and critic\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "        # Action and observation specs\n",
    "        self.action_spec = action_spec\n",
    "        self.observation_spec = observation_spec\n",
    "\n",
    "    def policy(self, observation):\n",
    "        action_type_logits, num_shares_logits = self.actor(observation)\n",
    "        action_type_probs = tf.nn.softmax(action_type_logits)\n",
    "        num_shares_probs = tf.nn.softmax(num_shares_logits)\n",
    "        return action_type_probs, num_shares_probs\n",
    "    \n",
    "    def sample_actions(self, action_type_probs, num_shares_probs):\n",
    "        action_type = np.random.choice(3, p=action_type_probs.numpy())\n",
    "        num_shares = np.random.choice(3, p=num_shares_probs.numpy())\n",
    "        return action_type, num_shares\n",
    "\n",
    "    def value(self, observation):\n",
    "        return self.critic(observation)\n",
    "\n",
    "    def compute_loss(self, rewards, values, log_probs, entropy, gamma=0.99):\n",
    "        # Calculate discounted rewards\n",
    "        discounted_rewards = []\n",
    "        running_add = 0\n",
    "        for reward in reversed(rewards):\n",
    "            running_add = reward + gamma * running_add\n",
    "            discounted_rewards.insert(0, running_add)\n",
    "        discounted_rewards = tf.convert_to_tensor(discounted_rewards)\n",
    "\n",
    "        # Compute critic loss\n",
    "        advantages = discounted_rewards - values\n",
    "        critic_loss = advantages ** 2\n",
    "\n",
    "        # Compute actor loss\n",
    "        actor_loss = -log_probs * tf.stop_gradient(advantages)\n",
    "        entropy_loss = -0.01 * entropy  # Encourage exploration\n",
    "        total_loss = actor_loss + 0.5 * critic_loss + entropy_loss\n",
    "\n",
    "        return tf.reduce_mean(total_loss)\n",
    "\n",
    "    def train_step(self, observations, actions, rewards, next_observations, dones):\n",
    "        with tf.GradientTape() as tape:\n",
    "            values = self.value(observations)\n",
    "            action_type_logits, num_shares_logits = self.actor(observations)\n",
    "            action_type_probs = tf.nn.softmax(action_type_logits)\n",
    "            num_shares_probs = tf.nn.softmax(num_shares_logits)\n",
    "            action_type_log_probs = tf.math.log(action_type_probs)\n",
    "            num_shares_log_probs = tf.math.log(num_shares_probs)\n",
    "            entropy = -tf.reduce_sum(action_type_probs * action_type_log_probs, axis=1) - \\\n",
    "                      tf.reduce_sum(num_shares_probs * num_shares_log_probs, axis=1)\n",
    "    \n",
    "            # Sampled actions log_probs\n",
    "            action_type_log_prob = tf.gather_nd(action_type_log_probs, tf.stack([tf.range(len(actions)), actions[:, 0]], axis=1))\n",
    "            num_shares_log_prob = tf.gather_nd(num_shares_log_probs, tf.stack([tf.range(len(actions)), actions[:, 1]], axis=1))\n",
    "    \n",
    "            next_values = self.value(next_observations)\n",
    "    \n",
    "            # Compute targets\n",
    "            targets = rewards + (1 - dones) * 0.99 * next_values\n",
    "    \n",
    "            # Calculate total loss\n",
    "            loss = self.compute_loss(rewards, values, action_type_log_prob + num_shares_log_prob, entropy)\n",
    "    \n",
    "        # Backpropagation\n",
    "        gradients = tape.gradient(loss, self.actor.trainable_variables + self.critic.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(gradients[:len(self.actor.trainable_variables)], self.actor.trainable_variables))\n",
    "        self.critic_optimizer.apply_gradients(zip(gradients[len(self.actor.trainable_variables):], self.critic.trainable_variables))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a3f2df-5295-40b1-9c0d-fb0b8f1900a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class Worker(threading.Thread):\n",
    "    def __init__(self, env, agent, global_episode_counter):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.global_episode_counter = global_episode_counter\n",
    "\n",
    "    def run(self):\n",
    "        while self.global_episode_counter < MAX_EPISODES:\n",
    "            observations = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                # Get the action probabilities for action type and number of shares\n",
    "                action_type_probs, num_shares_probs = self.agent.policy(observations)\n",
    "\n",
    "                # Sample an action tuple (action type, number of shares)\n",
    "                action_type, num_shares = self.agent.sample_actions(action_type_probs, num_shares_probs)\n",
    "\n",
    "                # Create action tuple\n",
    "                actions = np.array([action_type, num_shares])\n",
    "\n",
    "                next_observations, reward, done, _ = self.env.step(actions)\n",
    "\n",
    "                # Train step\n",
    "                self.agent.train_step(observations, actions, reward, next_observations, done)\n",
    "\n",
    "                observations = next_observations\n",
    "                total_reward += reward\n",
    "\n",
    "            self.global_episode_counter += 1\n",
    "            print(f\"Episode {self.global_episode_counter} Total Reward: {total_reward}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b53aa72-da77-4b4e-9e28-666d4ba8eb91",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'tarfile' from 'backports' (/opt/conda/lib/python3.11/site-packages/backports/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombined_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CombinedEnv\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m## Helper functions\u001b[39;00m\n\u001b[1;32m      5\u001b[0m PARAM_ENV_TYPE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcom\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/work/airtos/envs/combined_env.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas_ta\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mta\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrading_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TradingEnv\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas_ta/__init__.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_spec\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpkg_resources\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_distribution, DistributionNotFound\n\u001b[1;32m     10\u001b[0m _dist \u001b[38;5;241m=\u001b[39m get_distribution(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas_ta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Normalize case for Windows systems\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pkg_resources/__init__.py:95\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjaraco\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drop_comment, join_continuation, yield_lines\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplatformdirs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m user_cache_dir \u001b[38;5;28;01mas\u001b[39;00m _user_cache_dir\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/setuptools/_vendor/jaraco/text/__init__.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib_resources\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjaraco\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compose, method_cache\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjaraco\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExceptionTrap\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubstitution\u001b[39m(old, new):\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    Return a function that will perform a substitution on a string\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/setuptools/_vendor/jaraco/context.py:17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Iterator\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m12\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbackports\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tarfile\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtarfile\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'tarfile' from 'backports' (/opt/conda/lib/python3.11/site-packages/backports/__init__.py)"
     ]
    }
   ],
   "source": [
    "from utils import load_dataset\n",
    "from envs.combined_env import CombinedEnv\n",
    "\n",
    "## Helper functions\n",
    "PARAM_ENV_TYPE = 'com'\n",
    "\n",
    "def create_env(env_type: str, df, window_size, frame_bound):\n",
    "    if env_type == 'com':\n",
    "        return CombinedEnv(df=df, window_size=window_size, frame_bound=frame_bound)\n",
    "\n",
    "    raise NotImplementedError('unknown type')\n",
    "\n",
    "\n",
    "def create_training_envs(env_type: str):\n",
    "    ko_df = load_dataset('./resources/KO.csv')\n",
    "    amzn_df = load_dataset('./resources/AMZN.csv')\n",
    "    amd_df = load_dataset('./resources/AMD.csv')\n",
    "    pypl_df = load_dataset('./resources/PYPL.csv')\n",
    "    nflx_df = load_dataset('./resources/NFLX.csv')\n",
    "    window_size = 10\n",
    "\n",
    "    return [\n",
    "        # KO training envs\n",
    "        create_env(env_type, ko_df, window_size, (10, 120)),\n",
    "        create_env(env_type, ko_df, window_size, (120, 230)),\n",
    "        create_env(env_type, ko_df, window_size, (350, 470)),\n",
    "        create_env(env_type, ko_df, window_size, (1000, 1120)),\n",
    "        create_env(env_type, ko_df, window_size, (1700, 1820)),\n",
    "\n",
    "        # AMZN training envs\n",
    "        create_env(env_type, amzn_df, window_size, (10, 120)),\n",
    "        create_env(env_type, amzn_df, window_size, (120, 230)),\n",
    "        create_env(env_type, amzn_df, window_size, (350, 470)),\n",
    "        create_env(env_type, amzn_df, window_size, (1000, 1120)),\n",
    "        create_env(env_type, amzn_df, window_size, (1700, 1820)),\n",
    "\n",
    "        # AMD training envs\n",
    "        create_env(env_type, amd_df, window_size, (10, 120)),\n",
    "        create_env(env_type, amd_df, window_size, (120, 230)),\n",
    "        create_env(env_type, amd_df, window_size, (350, 470)),\n",
    "        create_env(env_type, amd_df, window_size, (1000, 1120)),\n",
    "        create_env(env_type, amd_df, window_size, (1700, 1820)),\n",
    "\n",
    "        # PYPL training envs\n",
    "        create_env(env_type, pypl_df, window_size, (10, 120)),\n",
    "        create_env(env_type, pypl_df, window_size, (120, 230)),\n",
    "        create_env(env_type, pypl_df, window_size, (350, 470)),\n",
    "        create_env(env_type, pypl_df, window_size, (1000, 1120)),\n",
    "        create_env(env_type, pypl_df, window_size, (1700, 1820)),\n",
    "\n",
    "        # NFLX training envs\n",
    "        create_env(env_type, nflx_df, window_size, (10, 120)),\n",
    "        create_env(env_type, nflx_df, window_size, (120, 230)),\n",
    "        create_env(env_type, nflx_df, window_size, (350, 470)),\n",
    "        create_env(env_type, nflx_df, window_size, (1000, 1120)),\n",
    "        create_env(env_type, nflx_df, window_size, (1700, 1820)),\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_testing_env(env_type: str):\n",
    "    ko_df = load_dataset('./resources/KO.csv')\n",
    "    window_size = 10\n",
    "    return create_env(env_type, ko_df, window_size, (2000, 2300))\n",
    "\n",
    "\n",
    "\n",
    "# ====================================== Create environments ======================================\n",
    "\n",
    "train_py_envs = create_training_envs(PARAM_ENV_TYPE)\n",
    "train_env_sample = tf_py_environment.TFPyEnvironment(train_py_envs[0])\n",
    "\n",
    "eval_py_env = create_testing_env(PARAM_ENV_TYPE)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4052db-63e8-4af9-a58f-73dd4a365c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start workers\n",
    "env = YourPyEnvironment()\n",
    "global_episode_counter = 0\n",
    "agent = A3CAgent(env.action_spec(), env.observation_spec())\n",
    "workers = [Worker(env, agent, global_episode_counter) for _ in range(num_workers)]\n",
    "\n",
    "for worker in workers:\n",
    "    worker.start()\n",
    "\n",
    "for worker in workers:\n",
    "    worker.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3569cee6-e9c3-4131-99d3-1b06d2b98468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
